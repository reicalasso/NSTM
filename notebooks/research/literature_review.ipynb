{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature Review for NSTM\n",
    "\n",
    "This notebook summarizes key papers relevant to the NSTM project, identifying their strengths, weaknesses, and potential gaps that NSTM aims to address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hybrid computing using a neural network with dynamic external memory (DNC)\n",
    "\n",
    "**Paper:** Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwi≈Ñska, A., ... & Hassabis, D. (2016). Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626), 471-476.\n",
    "\n",
    "**Objective / Problem:**\n",
    "To create a neural network that can learn algorithms by combining the pattern matching capabilities of neural networks with the algorithmic power of programmable computers through an external memory.\n",
    "\n",
    "**Architectural Details:**\n",
    "DNC extends Neural Turing Machines (NTMs) with a more sophisticated memory management system. Key components include:\n",
    "- **Controller Network:** An LSTM that interacts with the memory.\n",
    "- **Memory Matrix:** A 2D array where information is stored.\n",
    "- **Read/Write Heads:** Mechanisms to read from and write to the memory.\n",
    "- **Dynamic Memory Allocation:** A system to allocate and free memory locations.\n",
    "- **Temporal Linkage:** A mechanism to track the order of memory writes.\n",
    "- **Content-Based Addressing:** Finding memory locations based on content similarity.\n",
    "- **Location-Based Addressing:** Finding memory locations based on previous read/write positions.\n",
    "\n",
    "**Experimental Results:**\n",
    "DNC was tested on synthetic tasks such as copying long sequences, associative recall, and navigating graphs. It demonstrated the ability to learn and execute complex algorithms, outperforming LSTM baselines on these tasks.\n",
    "\n",
    "**Datasets:**\n",
    "Synthetic datasets for copying, associative recall, and graph traversal tasks.\n",
    "\n",
    "**Strengths and Innovations:**\n",
    "- Explicit external memory allows for storing and retrieving information.\n",
    "- Differentiable architecture enables end-to-end training.\n",
    "- Demonstrated ability to learn complex algorithms.\n",
    "- Dynamic memory allocation and temporal linkage provide sophisticated memory management.\n",
    "\n",
    "**Weaknesses and Limitations:**\n",
    "- Complex architecture with many interacting components, making training difficult.\n",
    "- Computationally expensive due to memory operations.\n",
    "- Slower execution times compared to simpler models.\n",
    "- Not optimized for long sequences in terms of memory efficiency.\n",
    "- Limited scalability to very large models and datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RWKV: Reinventing RNNs for the Transformer Era\n",
    "\n",
    "**Paper:** Peng, B., Chen, X., & Zhou, C. (2023). RWKV: Reinventing RNNs for the Transformer Era. arXiv preprint arXiv:2305.13048.\n",
    "\n",
    "**Objective / Problem:**\n",
    "To design an RNN architecture that can match the performance of Transformers while retaining the efficiency advantages of RNNs, such as linear time complexity and constant memory usage during inference.\n",
    "\n",
    "**Architectural Details:**\n",
    "RWKV is a linear RNN that uses a linear attention mechanism. Key components include:\n",
    "- **Token Shift Mechanism:** A technique to incorporate information from previous tokens.\n",
    "- **Receptance Weighted Key Value (RWKV) Operation:** A linear attention mechanism that combines keys, values, and a receptance vector.\n",
    "- **Channel Mixing and Time Mixing:** Two sub-blocks that operate on channels and time dimensions, respectively.\n",
    "- **Initialization Strategy:** Specific initialization to make the RNN behave like a Transformer at the beginning of training.\n",
    "\n",
    "**Experimental Results:**\n",
    "RWKV achieved competitive performance with Transformers on language modeling tasks, while being significantly faster in inference and using less memory. It demonstrated good scalability.\n",
    "\n",
    "**Datasets:**\n",
    "Language modeling datasets such as The Pile, WikiText-103, and others.\n",
    "\n",
    "**Strengths and Innovations:**\n",
    "- Linear time and memory complexity, making it efficient for long sequences.\n",
    "- Fast inference speed, constant with respect to sequence length.\n",
    "- Lower memory footprint compared to Transformers.\n",
    "- Can be initialized to behave like a Transformer.\n",
    "- Simpler architecture compared to DNC.\n",
    "\n",
    "**Weaknesses and Limitations:**\n",
    "- May suffer from approximation errors due to linear attention.\n",
    "- Less interpretable compared to explicit state models.\n",
    "- May not capture long-term dependencies as effectively as more complex models in all scenarios.\n",
    "- Not inherently designed for multimodal data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Efficiently Modeling Long Sequences with Structured State Spaces (S4)\n",
    "\n",
    "**Paper:** Gu, A., & Dao, T. (2022). Efficiently Modeling Long Sequences with Structured State Spaces. arXiv preprint arXiv:2111.00396.\n",
    "\n",
    "**Objective / Problem:**\n",
    "To develop a model that can efficiently process long sequences with linear time complexity, overcoming the quadratic complexity of Transformers.\n",
    "\n",
    "**Architectural Details:**\n",
    "S4 models use structured state space models derived from continuous-time linear dynamical systems. Key components include:\n",
    "- **State Space Model (SSM):** A continuous-time system described by state matrices A, B, and C.\n",
    "- **Discretization:** Techniques to convert continuous SSMs to discrete versions for practical implementation.\n",
    "- **Structured Matrices:** Use of structured matrices (e.g., diagonal plus low-rank) for efficient computation.\n",
    "- **HiPPO Matrices:** Special matrices for initializing the A matrix to capture history effectively.\n",
    "- **Layer Architecture:** S4 layers combined with non-linearities and other components.\n",
    "\n",
    "**Experimental Results:**\n",
    "S4 demonstrated excellent performance on long sequence modeling tasks, such as language modeling and image classification, with linear time complexity. It showed strong results on the Long Range Arena benchmark.\n",
    "\n",
    "**Datasets:**\n",
    "Long sequence datasets including text (WikiText-103), images (CIFAR-10, ImageNet), and audio.\n",
    "\n",
    "**Strengths and Innovations:**\n",
    "- Excellent performance on long sequence modeling.\n",
    "- Linear time and memory complexity.\n",
    "- Strong theoretical foundation in control theory and signal processing.\n",
    "- Fast training and inference.\n",
    "- Effective at capturing long-term dependencies.\n",
    "\n",
    "**Weaknesses and Limitations:**\n",
    "- Less interpretable due to complex mathematical foundations.\n",
    "- May require careful hyperparameter tuning.\n",
    "- Not inherently designed for multimodal data.\n",
    "- The discretization process can be complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyena Hierarchy: Towards Larger Convolutional Language Models\n",
    "\n",
    "**Paper:** Poli, M., Massaroli, S., Nguyen, E., Yoder, D., Zhang, H., Dao, T., ... & Ermon, S. (2023). Hyena Hierarchy: Towards Larger Convolutional Language Models. arXiv preprint arXiv:2302.10866.\n",
    "\n",
    "**Objective / Problem:**\n",
    "To design a convolutional architecture for long sequence modeling that is more efficient than attention-based models while maintaining competitive performance.\n",
    "\n",
    "**Architectural Details:**\n",
    "Hyena uses implicit convolutions with learnable filters. Key components include:\n",
    "- **Hyena Operator:** A combination of depthwise separable convolutions and data-controlled gating.\n",
    "- **Filter Function:** A neural network that generates convolutional filters.\n",
    "- **Data-Controlled Gating:** Mechanisms to modulate the convolution based on input data.\n",
    "- **Hierarchical Structure:** Use of multiple Hyena operators at different resolutions.\n",
    "\n",
    "**Experimental Results:**\n",
    "Hyena achieved strong performance on language modeling tasks with sub-quadratic time complexity. It demonstrated good scalability and efficiency.\n",
    "\n",
    "**Datasets:**\n",
    "Language modeling datasets such as WikiText-103, The Pile.\n",
    "\n",
    "**Strengths and Innovations:**\n",
    "- Efficient for long sequences with sub-quadratic complexity.\n",
    "- Strong performance on language modeling tasks.\n",
    "- Simpler architecture compared to attention mechanisms.\n",
    "- Data-controlled gating allows for flexible filtering.\n",
    "\n",
    "**Weaknesses and Limitations:**\n",
    "- May not be as flexible as attention mechanisms for certain tasks.\n",
    "- Convolutional nature might limit its ability to capture global dependencies as effectively as attention in some cases.\n",
    "- Less interpretable compared to explicit state models.\n",
    "- May require careful design of the filter function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scaling Laws for Neural Language Models\n",
    "\n",
    "**Paper:** Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... & Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.\n",
    "\n",
    "**Objective / Problem:**\n",
    "To investigate the scaling laws for large language models, examining how model performance improves with increases in model size, dataset size, and computational budget.\n",
    "\n",
    "**Architectural Details:**\n",
    "The paper focuses on standard Transformer architectures. It does not propose a new architecture but analyzes existing ones.\n",
    "\n",
    "**Experimental Results:**\n",
    "The paper provides empirical relationships showing how loss decreases with increased model size, dataset size, and compute. It suggests optimal allocation of resources.\n",
    "\n",
    "**Datasets:**\n",
    "Various language modeling datasets used for training large Transformers.\n",
    "\n",
    "**Strengths and Innovations:**\n",
    "- Provides valuable insights into how to scale models effectively.\n",
    "- Helps in resource allocation and model design decisions.\n",
    "- Demonstrates the importance of large-scale training.\n",
    "- Empirical validation of scaling laws.\n",
    "\n",
    "**Weaknesses and Limitations:**\n",
    "- Focuses on traditional Transformer architectures.\n",
    "- Does not address the efficiency or interpretability issues of large models.\n",
    "- May not directly apply to novel architectures like NSTM.\n",
    "- Does not consider the environmental impact of large-scale training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Retentive Network: A Successor to Transformer for Large Language Models\n",
    "\n",
    "**Paper:** Sun, Y., Geng, X., Zhang, S., Zhang, Y., Xu, Y., Wang, B., & Zheng, B. (2023). Retentive Network: A Successor to Transformer for Large Language Models. arXiv preprint arXiv:2307.08621.\n",
    "\n",
    "**Objective / Problem:**\n",
    "To propose a new architecture, RetNet, that retains the parallelizability of Transformers for training while being more efficient for inference, especially for long sequences.\n",
    "\n",
    "**Architectural Details:**\n",
    "RetNet introduces retention mechanisms as a replacement for self-attention. Key components include:\n",
    "- **Retention Mechanism:** A variant of attention with decay applied to past information.\n",
    "- **Chunkwise Retention:** A method to process sequences in chunks for efficient parallel training.\n",
    "- **Recurrent Processing:** A recurrent formulation for efficient inference.\n",
    "- **Hybrid Parallel Processing:** Combining parallel and recurrent processing for different stages.\n",
    "\n",
    "**Experimental Results:**\n",
    "RetNet achieved competitive performance with Transformers on language modeling tasks while offering significant speedups in inference, especially for long sequences. It demonstrated better efficiency in terms of FLOPs and memory usage.\n",
    "\n",
    "**Datasets:**\n",
    "Language modeling datasets such as The Pile, WikiText-103.\n",
    "\n",
    "**Strengths and Innovations:**\n",
    "- Retention mechanism provides a new way to model dependencies with decay.\n",
    "- Efficient inference through recurrent processing.\n",
    "- Parallel training via chunkwise retention.\n",
    "- Better FLOPs and memory efficiency compared to standard Transformers.\n",
    "- Maintains competitive performance.\n",
    "\n",
    "**Weaknesses and Limitations:**\n",
    "- The retention mechanism is a form of attention and may still have quadratic complexity in some formulations.\n",
    "- Requires careful implementation of chunkwise processing.\n",
    "- May not be as interpretable as models with explicit state management.\n",
    "- Limited public availability of code and models at the time of this review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Literature Gap Analysis\n",
    "\n",
    "**Unaddressed Problems:**\n",
    "- **Efficiency vs. Interpretability Trade-off:** Many efficient models (RWKV, S4, Hyena, RetNet) sacrifice interpretability. NSTM aims to bridge this gap by providing explicit state management.\n",
    "- **Explicit State Management for Long Sequences:** While DNCs have explicit memory, they are not efficient. Transformers lack explicit state. NSTM introduces efficient explicit state management.\n",
    "- **Multimodal Integration:** Most of the discussed models are primarily designed for sequential data. Extending them to multimodal data can be challenging. NSTM's modular design could facilitate multimodal integration.\n",
    "- **Dynamic State Complexity:** Existing models often have fixed computational paths. NSTM's adaptive state allocation aims to dynamically adjust computational resources.\n",
    "- **Memory Efficiency for Very Long Sequences:** While S4, Hyena, and RetNet address long sequences, NSTM aims to provide even better memory efficiency through its state management.\n",
    "\n",
    "**Gap Categories:**\n",
    "- **Performance:** Maintaining high performance while improving efficiency.\n",
    "- **Architecture:** Designing architectures that are both efficient and interpretable.\n",
    "- **Efficiency:** Reducing FLOPs, memory usage, and inference time.\n",
    "- **Flexibility:** Creating models that can easily adapt to different tasks and data types (multimodal).\n",
    "- **Scalability:** Ensuring models scale well with sequence length and model size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hypothesis List for NSTM\n",
    "\n",
    "Based on the literature review and gap analysis, the following hypotheses are formulated for NSTM:\n",
    "\n",
    "1.  **H1 (Long Sequence Efficiency):** NSTM will demonstrate superior efficiency (in terms of FLOPs and memory usage) compared to traditional Transformers on long sequence tasks, while maintaining competitive accuracy.\n",
    "    *Testable Experiment:* Compare NSTM and Transformer performance on Long Range Arena (LRA) benchmarks, measuring FLOPs, memory usage, and accuracy.\n",
    "\n",
    "2.  **H2 (Adaptive State Sparsity):** NSTM's dynamic state allocation and pruning mechanisms will lead to significant computational savings by reducing the number of active states for inputs that do not require full model capacity.\n",
    "    *Testable Experiment:* Monitor the number of active states during inference on varying complexity inputs and correlate with computational cost.\n",
    "\n",
    "3.  **H3 (Interpretability):** The explicit state management in NSTM will provide better interpretability of the model's decision-making process compared to black-box models like standard Transformers.\n",
    "    *Testable Experiment:* Analyze state activation patterns and importance scores to understand model behavior on specific tasks.\n",
    "\n",
    "4.  **H4 (Scalability):** NSTM will scale more effectively to very long sequences (e.g., >100k tokens) than quadratic-complexity models, with stable memory usage and performance.\n",
    "    *Testable Experiment:* Evaluate NSTM on synthetic tasks with increasing sequence lengths and measure memory usage and performance.\n",
    "\n",
    "5.  **H5 (Multimodal Potential):** The modular architecture of NSTM will facilitate easier integration of multimodal data compared to monolithic architectures.\n",
    "    *Testable Experiment:* Extend NSTM to a simple multimodal task (e.g., image captioning) and compare ease of implementation and performance with baseline models.\n",
    "\n",
    "6.  **H6 (Training Stability):** NSTM's gated state updates and dynamic management will lead to more stable training compared to complex memory-augmented models like DNC.\n",
    "    *Testable Experiment:* Compare training loss curves and gradient stability metrics of NSTM and DNC on the same tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparison Matrix\n",
    "\n",
    "This matrix compares the key models based on several important metrics. The ratings are qualitative (High, Medium, Low) for simplicity, but can be quantified with specific values from papers.\n",
    "\n",
    "| Model | Category | Loss | Accuracy | Token/s (Inference) | FLOPs | Memory Footprint | Interpretability |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| **Transformer (Baseline)** | Transformer Scaling | Low | High | Low | High | High | Low |\n",
    "| **DNC** | Bellek-Augmented | Medium-High | Medium | Low | High | High | Medium |\n",
    "| **RWKV** | RNN | Low | High | High | Low | Low | Low |\n",
    "| **S4** | State-Space | Low | High | High | Low | Low | Low |\n",
    "| **Hyena** | Hybrid | Low | High | High | Medium | Medium | Low |\n",
    "| **RetNet** | Hybrid | Low | High | Medium-High | Medium | Medium | Low |\n",
    "| **NSTM (Hypothesized)** | Hybrid | Low | High | High | Low | Low | High |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Trend Analysis\n",
    "\n",
    "**Emerging Architectural Approaches (Last 3-5 Years):**\n",
    "- **Linear Transformers and RNNs:** Models like RWKV have shown that linear attention mechanisms can be effective, combining the efficiency of RNNs with the performance of Transformers.\n",
    "- **State Space Models:** S4 and related models have demonstrated the power of continuous-time dynamical systems for sequence modeling, offering linear complexity.\n",
    "- **Convolutional Models:** Hyena and similar models have revisited convolutional approaches, using data-controlled filtering for long sequences.\n",
    "- **Hybrid Models:** RetNet and others have combined parallel and recurrent processing to achieve both training efficiency and inference speed.\n",
    "- **Explicit Memory and State Management:** While not new, there's renewed interest in models with explicit memory (like DNC) and state management, which NSTM aims to modernize and make more efficient.\n",
    "\n",
    "**Key Trends:**\n",
    "- **Efficiency:** A strong focus on reducing computational and memory costs, especially for long sequences.\n",
    "- **Scalability:** Developing models that scale well with sequence length and model size.\n",
    "- **Inference Speed:** Designing models that are not only efficient to train but also fast during inference.\n",
    "- **Theoretical Foundations:** Increasing use of strong theoretical foundations from control theory, signal processing, and dynamical systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Benchmark Comparisons\n",
    "\n",
    "Comparing models on standard benchmarks provides insights into their relative strengths.\n",
    "\n",
    "**Long Range Arena (LRA):**\n",
    "- **S4** has shown strong performance on LRA, often leading in several tasks.\n",
    "- **Hyena** also performed well on LRA, demonstrating competitive results.\n",
    "- **RWKV** and **RetNet** are newer and may not have extensive LRA results yet, but early results are promising.\n",
    "- **DNC** was not designed for LRA, but its successors and related models are being evaluated.\n",
    "\n",
    "**Language Modeling (e.g., WikiText-103, The Pile):**\n",
    "- **RWKV** has shown competitive perplexity scores with Transformers.\n",
    "- **RetNet** also demonstrated strong language modeling performance.\n",
    "- **S4** and **Hyena** have shown good results, though they may require more tuning for language-specific tasks.\n",
    "\n",
    "**Image Classification (e.g., ImageNet, CIFAR-10):**\n",
    "- **S4** has been applied to image classification and shown promising results.\n",
    "- **Hyena** has also been used for image tasks.\n",
    "- Traditional models like **Transformers** (Vision Transformers) remain strong baselines for image tasks.\n",
    "\n",
    "**Key Insight:** The choice of model often depends on the specific task and requirements (e.g., efficiency vs. accuracy). NSTM aims to offer a good balance across these dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Open-Source Code and Pre-trained Models\n",
    "\n",
    "Access to code and pre-trained models is crucial for research and development.\n",
    "\n",
    "- **DNC:** Original code is available from DeepMind, but it's complex. Community implementations exist.\n",
    "- **RWKV:** Highly open-source with active development. Code and pre-trained models are available on GitHub.\n",
    "- **S4:** Open-source implementations are available. Pre-trained models are also shared by the community.\n",
    "- **Hyena:** Code is available from the authors. Pre-trained models are being developed.\n",
    "- **RetNet:** Code release was limited at the time of this review, but community implementations are emerging.\n",
    "\n",
    "**For NSTM:**\n",
    "- NSTM will be developed as a fully open-source project.\n",
    "- Code, pre-trained models, and comprehensive documentation will be made available.\n",
    "- This will facilitate community adoption and contributions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}